id: "batch_dataflow"

dataflow:
  project: "dhuodata"
  runner: "DataflowRunner" # SparkRunner (Oracle)
  region: "us-west1"
  staging_location: "gs://petrobras-datalake/temp"
  temp_location: "gs://petrobras-datalake/temp"
  template_location: "gs://petrobras-datalake/template/job_ingestion_voos"
  save_main_session: False
  subnetwork: "https://www.googleapis.com/compute/v1/projects/dhuodata/regions/us-west1/subnetworks/dhuodata-subnet-stg-cluster"
  requirements_file: "requirements.txt"

inputs:
  voos:
    # file: "./data/raw/voos_sample.csv"
    file: "gs://petrobras-datalake/inputs/voos_sample.csv"
    separator: ","

outputs:
  voos:
    # file: "./data/processed/voos"
    file: "gs://petrobras-datalake/outputs/voos2.csv"        
    header: ""
